---
title: "Random forests"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(tidymodels)
library(rpart)
library(ranger)
knitr::opts_chunk$set(echo = FALSE)

set.seed(1234)

plot_tuning_metrics <- function(tuning_results, hyperparameter, multiple = FALSE){
  
  if(multiple == FALSE) {
    
    tuning_results %>%
      collect_metrics() %>%
      ggplot(aes(x = eval(parse(text = hyperparameter)), y = mean, color = .metric)) +
      geom_errorbar(aes(
        ymin = mean - std_err,
        ymax = mean + std_err
      ),
      alpha = 0.5
      ) +
      geom_line(size = 1.5) +
      facet_wrap(~.metric, scales = "free", nrow = 2) +
      theme_bw() +
      theme(legend.position = "none") +
      xlab(hyperparameter)
    
  } else {
    
    tuning_results %>% 
      collect_metrics() %>%
      ggplot(aes(x = eval(parse(text = hyperparameter)), y = mean)) +
      geom_point() +
      facet_wrap(~.metric, scales = "free", nrow = 2) +
      theme_bw()  +
      xlab(hyperparameter)
    
  }
  
}

turbines <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-27/wind-turbine.csv")

turbines_df <- turbines %>%
  transmute(
    turbine_capacity = turbine_rated_capacity_k_w,
    rotor_diameter_m,
    hub_height_m,
    commissioning_date = parse_number(commissioning_date),
    province_territory = fct_lump_n(province_territory, 10),
    model = fct_lump_n(model, 10)
  ) %>%
  filter(!is.na(turbine_capacity)) %>%
  mutate_if(is.character, factor)

turbines_recoded <- turbines_df %>% 
  mutate(turbine_capacity = ifelse(turbine_capacity > 2300, "high", "other")) %>% 
  mutate(turbine_capacity = as.factor(turbine_capacity)) 

turbines_split <- initial_split(turbines_recoded,
                                 prop = 0.75,
                                 strata = "turbine_capacity")

train <- training(turbines_split)
test <- testing(turbines_split)

turbines_recipe <- recipe(turbine_capacity ~ ., data = turbines_recoded) 

metrics_list <- metric_set(accuracy, precision, recall, roc_auc)

turbines_folds <- vfold_cv(train, v = 3,
                            strata = "turbine_capacity")


rf_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(turbines_recipe)

rf_grid <- grid_random(
  parameters(mtry() %>% range_set(c(4, 12)), trees(), min_n()), 
  size = 5)

rf_cv_results <- rf_wf %>% 
  tune_grid(resamples = turbines_folds,
            grid = rf_grid,
            metrics = metrics_list)

best_rf_model <- rf_cv_results %>% 
  select_best(metric = "roc_auc")

rf_final_fit <- rf_wf %>% 
  finalize_workflow(best_rf_model) %>% 
  last_fit(split = turbines_split)

rf_predictions <- rf_final_fit %>% 
  collect_predictions()

rf_final_perf <- metrics_list(rf_predictions,
                                truth = turbine_capacity,
                                estimate = .pred_class,
                                .pred_high)

```

## Introduction

Today we are going to try using decision trees and random forests. We are also going to use a new dataset of wind turbines in Canada which we have imported called ``turbines_df``. You should have done the exercise in the ``decision_trees``training. Now, apply it to a random forest model.


## Random forest

First, split the data between a training and test set, allocating 75% percent of the data to training, and stratifying by ``turbine_capacity``:

```{r rf1, exercise=TRUE}


```

```{r rf1-solution}

turbines_split <- initial_split(turbines_recoded,
                                 prop = 0.75,
                                 strata = turbine_capacity)

train <- training(turbines_split)
test <- testing(turbines_split)

```

Create the 'recipe' by using ``turbine_capacity`` as the predictor and all other variables as the features:

```{r rf2, exercise=TRUE}


```

```{r rf2-solution}

turbines_recipe <- recipe(turbine_capacity ~ ., data = turbines_recoded) 

```

Create an object to store performance metrics. Use ``accuracy``, ``precision``, ``recall``, ``roc_auc``:

```{r rf3, exercise=TRUE}


```

```{r rf3-solution}

metrics_list <- metric_set(accuracy, precision, recall, roc_auc)

```

Tell R to create a model. Use the code you wrote to create ``dt_model``, but with the following changes:

* Call the model ``rf_model`` instead of ``dt_model``;
* Use the ``rand_forest()`` function instead of ``logistic_reg()`` to tell R this is a random forest model;
* Tune the following hyperparameters: ``mtry()``, ``trees()``, ``min_n()``
* Use the ``"ranger"`` package instead of the ``"rpart"`` package;

```{r rf4, exercise=TRUE}


```

```{r rf4-solution}

rf_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

```

Create a workflow. Call it ``rf_wf`` instead of ``dt_wf``:

```{r rf5, exercise=TRUE}


```

```{r rf5-solution}

rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(turbines_recipe)

```

Create your cross-validation folds. Use 3-fold cross-validation.

```{r rf6, exercise=TRUE}


```

```{r rf6-solution}

turbines_folds <- vfold_cv(train, v = 3,
                            strata = "turbine_capacity")

```

Create a random grid of size 20 to tune your hyperparameters. Call your new object rf_grid. Also, instead of just writing ``mtry()``, use ``mtry() %>% range_set(c(4, 12))`` (because for some reason this needs to be specified or else it will not work):

```{r rf7, exercise=TRUE}


```

```{r rf7-solution}

rf_grid <- grid_random(
  parameters(mtry() %>% range_set(c(4, 12)), trees(), min_n()), 
  size = 20)

```

Tune your hyperparameters, saving your results as ``rf_cv_results``:

```{r rf8, exercise=TRUE}


```

```{r rf8-solution}

rf_cv_results <- rf_wf %>% 
  tune_grid(resamples = turbines_folds,
            grid = rf_grid,
            metrics = metrics_list)

```

Plot the performance for different values of ``mtry()`` using the ``plot_tuning_metrics()`` function:

```{r rf9, exercise=TRUE}


```

```{r rf9-solution}

rf_cv_results %>% 
  plot_tuning_metrics(hyperparameter = "mtry", multiple = TRUE) 

```



Plot the performance for different values of ``trees()`` using the ``plot_tuning_metrics()`` function:

```{r rf10, exercise=TRUE}


```

```{r rf10-solution}

rf_cv_results %>% 
  plot_tuning_metrics(hyperparameter = "trees", multiple = TRUE) 

```


Plot the performance for different values of ``min_n()`` using the ``plot_tuning_metrics()`` function:

```{r rf11, exercise=TRUE}


```

```{r rf11-solution}

rf_cv_results %>% 
  plot_tuning_metrics(hyperparameter = "min_n", multiple = TRUE) 

```

Select the best model using ``select_best()``, using AUC as the performance metric:

```{r rf12, exercise=TRUE}


```

```{r rf12-solution}

best_rf_model <- rf_cv_results %>% 
  select_best(metric = "roc_auc")

```

Apply your best model to the data. Save the final fit as ``rf_final_fit`:

```{r rf13, exercise=TRUE}


```

```{r rf13-solution}

rf_final_fit <- rf_wf %>% 
  finalize_workflow(best_rf_model) %>% 
  last_fit(split = turbines_split)

```

Collect the predictions and save them as ``rf_predictions``:

```{r rf14, exercise=TRUE}


```

```{r rf14-solution}

rf_predictions <- rf_final_fit %>% 
  collect_predictions()

```

Create a confusion matrix with a heatmap:

```{r rf15, exercise=TRUE}


```

```{r rf15-solution}

conf_mat(rf_predictions,
         truth = turbine_capacity,
         estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

Draw a ROC curve:

```{r rf16, exercise=TRUE}


```

```{r rf16-solution}

rf_predictions %>% 
  roc_curve(truth = turbine_capacity, .pred_high) %>% 
  autoplot()

```

Collect the list of metrics using ``metrics_list()`` and save it as ``dt_final_perf``:

```{r rf17, exercise=TRUE}


```

```{r rf17-solution}

rf_final_perf <- metrics_list(rf_predictions,
                                truth = turbine_capacity,
                                estimate = .pred_class,
                                .pred_high)

```

Print your final performance metrics:

```{r rf18, exercise=TRUE}


```

```{r rf18-solution}

rf_final_perf

```

Which model has the best performance on this data? Random forest or a decision tree?

